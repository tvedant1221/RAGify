# RAGify
Ask questions from any PDF using AI â€“ RAG system powered by Mistral-7B, FAISS, and LangChain.

âœ… Built with:

ğŸ” FAISS for efficient similarity search
ğŸ“„ PyMuPDF for PDF parsing
ğŸ¤— Hugging Face Transformers for the Mistral-7B-Instruct model
ğŸ§  LangChain for easy chaining and orchestration
âš¡ï¸ Google Colab GPU for performance
ğŸŒ (Optional) Streamlit or Gradio UI
ğŸ’¾ Vectorstore persistence for efficient reuse



âœ¨ Features
ğŸ“¥ Upload any PDF and ask context-aware questions
ğŸ” Uses embeddings + similarity search (FAISS)
ğŸ§  Answers generated by Mistral-7B (via Hugging Face)
ğŸ” Multi-turn conversation support
ğŸ’¾ Saves vectorstore for faster future queries
ğŸš€ Deployable via Render/Streamlit/Hugging Face Spaces



ğŸ› ï¸ Setup (Local or Colab)
1. Clone this repo: 
	git clone https://github.com/yourusername/rag-pdf-mistral.git
	cd rag-pdf-mistral

2. Install dependencies:
	pip install -r requirements.txt

3. Run the app:
	streamlit run app.py

Or open and run the Colab notebook directly.



ğŸ” Security Note
For private models like Mistral-7B-Instruct, you must create a Hugging Face token and authenticate securely using getpass, .env, or GitHub secrets. Never hardcode your token into code.


ğŸ§  Model
ğŸ”— mistralai/Mistral-7B-Instruct-v0.1
Requires access & runs best on GPU (Colab recommended)



ğŸ“‚ Folder Structure::
	ğŸ“ rag-pdf-mistral
	â”œâ”€â”€ app.py            # Main app (Streamlit or Gradio)
	â”œâ”€â”€ rag_pipeline.ipynb  # Colab version
	â”œâ”€â”€ utils/            # Helper functions
	â”œâ”€â”€ requirements.txt  # Python dependencies
	â””â”€â”€ README.md


ğŸ’¬ Contact
Feel free to open issues or reach out via GitHub Discussions for questions, suggestions, or contributions!
