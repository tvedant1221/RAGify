# RAGify
Ask questions from any PDF using AI – RAG system powered by Mistral-7B, FAISS, and LangChain.

✅ Built with:

🔍 FAISS for efficient similarity search
📄 PyMuPDF for PDF parsing
🤗 Hugging Face Transformers for the Mistral-7B-Instruct model
🧠 LangChain for easy chaining and orchestration
⚡️ Google Colab GPU for performance
🌐 (Optional) Streamlit or Gradio UI
💾 Vectorstore persistence for efficient reuse



✨ Features
📥 Upload any PDF and ask context-aware questions
🔍 Uses embeddings + similarity search (FAISS)
🧠 Answers generated by Mistral-7B (via Hugging Face)
🔁 Multi-turn conversation support
💾 Saves vectorstore for faster future queries
🚀 Deployable via Render/Streamlit/Hugging Face Spaces



🛠️ Setup (Local or Colab)
1. Clone this repo: 
	git clone https://github.com/yourusername/rag-pdf-mistral.git
	cd rag-pdf-mistral

2. Install dependencies:
	pip install -r requirements.txt

3. Run the app:
	streamlit run app.py

Or open and run the Colab notebook directly.



🔐 Security Note
For private models like Mistral-7B-Instruct, you must create a Hugging Face token and authenticate securely using getpass, .env, or GitHub secrets. Never hardcode your token into code.


🧠 Model
🔗 mistralai/Mistral-7B-Instruct-v0.1
Requires access & runs best on GPU (Colab recommended)



📂 Folder Structure::
	📁 rag-pdf-mistral
	├── app.py            # Main app (Streamlit or Gradio)
	├── rag_pipeline.ipynb  # Colab version
	├── utils/            # Helper functions
	├── requirements.txt  # Python dependencies
	└── README.md


💬 Contact
Feel free to open issues or reach out via GitHub Discussions for questions, suggestions, or contributions!
