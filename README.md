# RAGify
Ask questions from any PDF using AI – RAG system powered by Mistral-7B, FAISS, and LangChain.

✅ Built with:

1. 🔍 FAISS for efficient similarity search
2. 📄 PyMuPDF for PDF parsing
3. 🤗 Hugging Face Transformers for the Mistral-7B-Instruct model
4. 🧠 LangChain for easy chaining and orchestration
5. ⚡️ Google Colab GPU for performance
6. 🌐 (Optional) Streamlit or Gradio UI
7. 💾 Vectorstore persistence for efficient reuse



✨ Features
1. 📥 Upload any PDF and ask context-aware questions
2. 🔍 Uses embeddings + similarity search (FAISS)
3. 🧠 Answers generated by Mistral-7B (via Hugging Face)
4. 🔁 Multi-turn conversation support
5. 💾 Saves vectorstore for faster future queries
6. 🚀 Deployable via Render/Streamlit/Hugging Face Spaces



🛠️ Setup (Local or Colab)
1. Clone this repo: 
	git clone https://github.com/yourusername/rag-pdf-mistral.git
	cd rag-pdf-mistral

2. Install dependencies:
	pip install -r requirements.txt

3. Run the app:
	streamlit run app.py

Or open and run the Colab notebook directly.



🔐 Security Note
For private models like Mistral-7B-Instruct, you must create a Hugging Face token and authenticate securely using getpass, .env, or GitHub secrets. Never hardcode your token into code.


🧠 Model
🔗 mistralai/Mistral-7B-Instruct-v0.1
Requires access & runs best on GPU (Colab recommended)


💬 Contact
Feel free to open issues or reach out via GitHub Discussions for questions, suggestions, or contributions!
